{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = torchvision.models.mobilenet_v2(pretrained=True).features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone.out_channels = 1280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((32,), (64,), (128,), (256,), (512,))\n",
      "((0.5, 1.0, 2.0), (0.5, 1.0, 2.0), (0.5, 1.0, 2.0), (0.5, 1.0, 2.0), (0.5, 1.0, 2.0))\n"
     ]
    }
   ],
   "source": [
    "anchor_generator = AnchorGenerator(sizes=((32,64,128,256,512)),\n",
    "                                  aspect_ratios=((0.5,1.0,2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 3, 3, 3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_generator.num_anchors_per_location()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "print(anchor_generator.num_anchors_per_location())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "                                                output_size=7,sampling_ratio=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FasterRCNN(backbone,\n",
    "                  num_classes=10,\n",
    "                  rpn_anchor_generator=anchor_generator,\n",
    "                  box_roi_pool=roi_pooler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "x = [torch.rand(3,200,400),torch.rand(3,500,400)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 200, 400])\n",
      "torch.Size([3, 666, 1333])\n",
      "torch.Size([3, 500, 400])\n",
      "torch.Size([3, 1000, 800])\n",
      "(3, 1000, 1333)\n",
      "(2, 3, 1024, 1344)\n",
      "images.tensors.shape: torch.Size([2, 3, 1024, 1344])\n",
      "feature.shape torch.Size([2, 1280, 32, 42])\n",
      "features: torch.Size([2, 1280, 32, 42])\n",
      "shape(features[0]): torch.Size([2, 1280, 32, 42])\n",
      "grid_sizes: (torch.Size([32, 42]),)\n",
      "strides: ((32.0, 32.0),)\n",
      "[tensor([[-23., -11.,  23.,  11.],\n",
      "        [-16., -16.,  16.,  16.],\n",
      "        [-11., -23.,  11.,  23.]]), tensor([[-45., -23.,  45.,  23.],\n",
      "        [-32., -32.,  32.,  32.],\n",
      "        [-23., -45.,  23.,  45.]]), tensor([[-91., -45.,  91.,  45.],\n",
      "        [-64., -64.,  64.,  64.],\n",
      "        [-45., -91.,  45.,  91.]]), tensor([[-181.,  -91.,  181.,   91.],\n",
      "        [-128., -128.,  128.,  128.],\n",
      "        [ -91., -181.,   91.,  181.]]), tensor([[-362., -181.,  362.,  181.],\n",
      "        [-256., -256.,  256.,  256.],\n",
      "        [-181., -362.,  181.,  362.]])]\n",
      "shifts.shape: torch.Size([1344, 4])\n",
      "base_anchors: tensor([[-23., -11.,  23.,  11.],\n",
      "        [-16., -16.,  16.,  16.],\n",
      "        [-11., -23.,  11.,  23.]])\n",
      "return anchors.len: 1\n",
      "anchors.len: 2\n",
      "anchors[0].shape: torch.Size([4032, 4])\n",
      "anchors[0][100] tensor([1040.,  -16., 1072.,   16.])\n",
      "anchors[1][100] tensor([1040.,  -16., 1072.,   16.])\n",
      "objectness[0].shape: torch.Size([2, 3, 32, 42])\n",
      "pred_bbox_deltas[0].shape: torch.Size([2, 12, 32, 42])\n",
      "[4032]\n",
      "in concat:\n",
      "N: 2 A: 3 C: 1 H: 32 W: 42\n",
      "after concat,objectness.shape: torch.Size([8064, 1])\n",
      "after concat,pred_bbox_deltas.shape; torch.Size([8064, 4])\n",
      "before decode\n",
      "boxes_per_image: [4032, 4032]\n",
      "concat_boxes.shape: torch.Size([8064, 4])\n",
      "in decode_single:\n",
      "rel_codes.shape: torch.Size([8064, 4])\n",
      "rel_codes[100:105,:]: tensor([[ 0.1386,  0.0011,  0.1235,  0.1077],\n",
      "        [-0.0091,  0.3391,  0.1599, -0.0972],\n",
      "        [-0.2281, -0.0960,  0.0280,  0.0026],\n",
      "        [ 0.0810, -0.0276,  0.1425,  0.1783],\n",
      "        [-0.0177,  0.3243,  0.1184, -0.0872]])\n",
      "boxes.shape: torch.Size([8064, 4])\n",
      "wx: 1.0 ,wy: 1.0 ,ww: 1.0 ,wh: 1.0\n",
      "pred_boxes.shape: torch.Size([8064, 4])\n",
      "after decode\n",
      "proposals.shape: torch.Size([8064, 1, 4])\n",
      "num_anchors_per_level: [4032]\n",
      "len(levels): 1\n",
      "levels.shape: torch.Size([4032])\n",
      "after reshape,levels.shape: torch.Size([2, 4032])\n",
      "levels[:,:10]: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "ob.shape: torch.Size([2, 4032])\n",
      "pre_nms_top_n: 1000\n",
      "keep.len: 676\n",
      "keep.len: 676\n",
      "keep.len: 480\n",
      "keep.len: 480\n",
      "proposals[0].shape: torch.Size([676, 4])\n",
      "proposals[1].shape: torch.Size([480, 4])\n",
      "proposals[0][:2,:]: tensor([[1030.2184,   21.8239, 1071.6378,   44.2340],\n",
      "        [1058.6002,   19.2188, 1100.6835,   42.5692]])\n",
      "image_sizes: [torch.Size([666, 1333]), torch.Size([1000, 800])]\n",
      "features[0].shape: torch.Size([2, 1280, 32, 42])\n",
      "before box_roi_pool,features[0].shape torch.Size([2, 1280, 32, 42])\n",
      "concat_boxes.shape: torch.Size([1156, 4])\n",
      "ids.shape: torch.Size([1156, 1])\n",
      "ids[:10,0]: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "ids[-10:,0]: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "rois.shape: torch.Size([1156, 5])\n",
      "rois[:5,:]: tensor([[   0.0000, 1030.2184,   21.8239, 1071.6378,   44.2340],\n",
      "        [   0.0000, 1058.6002,   19.2188, 1100.6835,   42.5692],\n",
      "        [   0.0000, 1029.4694,   52.0272, 1071.4379,   74.8777],\n",
      "        [   0.0000, 1224.5559,  628.6553, 1271.9128,  654.0674],\n",
      "        [   0.0000,  230.2514,  659.4517,  276.8404,  666.0000]])\n",
      "original_input_shape: (1000, 1333)\n",
      "scales: [0.03125]\n",
      "lvl_min: 5.0\n",
      "lvl_max: 5.0\n",
      "self.scales: [0.03125]\n",
      "num_levels: 1\n",
      "after box_roi_pool,box_feature.shape torch.Size([1156, 1280, 7, 7])\n",
      "after box_head, box_features.shape: torch.Size([1156, 1024])\n",
      "boxes_per_image: [676, 480]\n",
      "concat_boxes.shape: torch.Size([1156, 4])\n",
      "in decode_single:\n",
      "rel_codes.shape: torch.Size([1156, 40])\n",
      "rel_codes[100:105,:]: tensor([[ 3.9378e-02, -4.5796e-02, -1.6148e-02, -1.0557e-01,  9.3050e-02,\n",
      "          3.6208e-02, -2.8366e-02, -1.4924e-01, -3.7270e-02,  2.9999e-03,\n",
      "         -8.6810e-02, -4.6607e-02, -4.6126e-02, -1.6499e-02, -1.7577e-02,\n",
      "         -1.6539e-02,  1.3640e-02, -4.8417e-02, -6.1153e-02,  2.5365e-02,\n",
      "         -2.6186e-02,  3.1757e-02,  9.6367e-02, -7.3486e-02, -6.9708e-02,\n",
      "          6.4291e-02,  8.0725e-02,  8.1883e-03,  1.9482e-02,  6.8865e-02,\n",
      "         -7.7043e-02,  2.4280e-02,  6.3018e-02, -8.1585e-02, -4.0485e-02,\n",
      "          2.0261e-02,  2.1326e-01,  4.8771e-02, -1.4320e-02, -2.6314e-04],\n",
      "        [-3.0198e-02,  3.6293e-02, -9.0017e-03, -1.0306e-01,  1.3678e-01,\n",
      "         -3.0294e-02, -2.5757e-02, -9.1741e-02, -3.6329e-02,  4.1787e-02,\n",
      "         -8.2944e-02, -5.4220e-02, -7.2151e-02, -1.6541e-03, -5.1935e-02,\n",
      "         -5.0231e-03,  5.3728e-02, -5.0089e-02, -7.8670e-02,  3.3384e-02,\n",
      "         -1.3914e-03,  8.9232e-03, -1.0213e-02, -2.5251e-02,  2.4754e-02,\n",
      "          4.8429e-02, -7.4667e-02,  2.0462e-02, -3.6029e-02,  4.0372e-02,\n",
      "         -1.0703e-01,  1.1260e-02, -2.5385e-02,  3.2366e-02, -1.9461e-02,\n",
      "          8.9548e-03,  1.5604e-01, -2.8081e-02,  1.2099e-02, -5.6095e-02],\n",
      "        [ 4.6718e-04,  3.3466e-02,  2.0034e-02, -1.3279e-01,  1.4168e-01,\n",
      "          1.1721e-02, -2.5193e-02, -7.3265e-02, -1.1902e-02,  4.3031e-02,\n",
      "         -1.0001e-01, -3.6972e-02, -9.1374e-02,  1.1306e-02, -7.5379e-02,\n",
      "          2.7467e-02,  5.0903e-02, -2.9583e-02, -6.6272e-02,  6.0529e-02,\n",
      "         -1.0439e-02,  2.5252e-02,  1.9122e-02, -1.5300e-02,  1.3440e-02,\n",
      "          3.8006e-02, -6.3221e-02, -1.5697e-02, -1.6379e-02,  2.3364e-02,\n",
      "         -9.4835e-02,  2.4691e-02, -3.5671e-04,  3.8839e-02, -4.1679e-02,\n",
      "         -1.0315e-02,  1.2717e-01, -2.6644e-02,  1.0195e-02,  7.0294e-03],\n",
      "        [-5.8813e-03,  2.6287e-02,  1.8319e-02, -1.6721e-01,  1.4885e-01,\n",
      "          1.7207e-02, -2.0929e-02, -1.3230e-01,  3.1813e-02,  5.1087e-02,\n",
      "         -6.7437e-02, -7.3210e-02, -8.9775e-02,  9.9804e-04, -9.0393e-02,\n",
      "          2.0070e-02,  6.0173e-02, -4.0234e-02, -6.8961e-02,  7.5585e-02,\n",
      "         -5.2536e-04, -2.1029e-03,  1.7457e-02,  6.1494e-05, -2.1748e-02,\n",
      "          4.6028e-02, -6.7532e-02, -1.8588e-02, -3.0755e-02,  5.5047e-02,\n",
      "         -5.8095e-02,  2.9835e-02,  1.8320e-02,  1.0607e-02, -3.9236e-02,\n",
      "         -4.5136e-04,  1.5479e-01, -6.6563e-02, -2.5736e-03,  2.1041e-02],\n",
      "        [-2.1946e-02,  1.0889e-02, -1.2418e-03, -1.4279e-01,  1.4787e-01,\n",
      "          3.6280e-03, -3.3229e-02, -9.4954e-02,  1.2635e-02,  3.6389e-02,\n",
      "         -1.0228e-01, -5.4571e-02, -9.5593e-02,  2.3301e-02, -6.7157e-02,\n",
      "         -2.3454e-02,  4.2030e-02, -1.5218e-02, -3.9181e-02,  4.3233e-02,\n",
      "         -7.0825e-03, -5.1395e-03,  2.4471e-02, -1.5872e-02, -2.0316e-02,\n",
      "          2.6883e-02, -5.9129e-02, -1.6607e-02, -1.8480e-02,  4.4438e-02,\n",
      "         -6.6559e-02, -4.4958e-03, -1.5435e-02,  3.9514e-02, -1.0811e-02,\n",
      "         -1.0520e-02,  1.3612e-01, -3.7863e-02,  1.3670e-02,  1.3744e-03]],\n",
      "       grad_fn=<SliceBackward>)\n",
      "boxes.shape: torch.Size([1156, 4])\n",
      "wx: 10.0 ,wy: 10.0 ,ww: 5.0 ,wh: 5.0\n",
      "pred_boxes.shape: torch.Size([1156, 40])\n",
      "after postprocess,boxes.len: 2\n",
      "boxes[0].shape: torch.Size([100, 4])\n"
     ]
    }
   ],
   "source": [
    "predictions = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_sizes = ((32,), (64,), (128,), (256,), (512,))\n",
    "aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0.5, 1.0, 2.0), (0.5, 1.0, 2.0), (0.5, 1.0, 2.0), (0.5, 1.0, 2.0), (0.5, 1.0, 2.0))\n"
     ]
    }
   ],
   "source": [
    "print(aspect_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n",
      "(2, 4)\n",
      "(3, 5)\n"
     ]
    }
   ],
   "source": [
    "a = (1,2,3)\n",
    "b = [3,4,5,6]\n",
    "for i in zip(a,b):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "strids = ((32,32),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = [torch.rand((3,4)),torch.rand((3,4)),torch.rand((3,4))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5, 9]\n"
     ]
    }
   ],
   "source": [
    "print(a[0::4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.135166556742356"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.log(1000./16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(62.5000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(torch.tensor(4.135166))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-5.)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(32./1000).log2().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
